{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96d7078d-94cb-4e1d-9192-a74519b31c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import PIL.Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import IPython.display as display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc4c9fda-7da8-45b2-8174-eaacdc5fc997",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(gpus[0:2],'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bdc4a98-4189-41ae-9935-19f5a4dfc8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFRecordExporter:\n",
    "    def __init__(self, tfrecord_dir, expected_images, print_progress=True, progress_interval=10):\n",
    "        self.tfrecord_dir       = tfrecord_dir\n",
    "        self.tfr_prefix         = os.path.join(self.tfrecord_dir, os.path.basename(self.tfrecord_dir))\n",
    "        self.expected_images    = expected_images\n",
    "        self.cur_images         = 0\n",
    "        self.shape              = None\n",
    "        self.resolution_log2    = None\n",
    "        self.tfr_writers        = []\n",
    "        self.print_progress     = print_progress\n",
    "        self.progress_interval  = progress_interval\n",
    "\n",
    "        if self.print_progress:\n",
    "            print('Creating dataset \"%s\"' % tfrecord_dir)\n",
    "        if not os.path.isdir(self.tfrecord_dir):\n",
    "            os.makedirs(self.tfrecord_dir)\n",
    "        assert os.path.isdir(self.tfrecord_dir)\n",
    "\n",
    "    def close(self):\n",
    "        if self.print_progress:\n",
    "            print('%-40s\\r' % 'Flushing data...', end='', flush=True)\n",
    "        for tfr_writer in self.tfr_writers:\n",
    "            tfr_writer.close()\n",
    "        self.tfr_writers = []\n",
    "        if self.print_progress:\n",
    "            print('%-40s\\r' % '', end='', flush=True)\n",
    "            print('Added %d images.' % self.cur_images)\n",
    "\n",
    "    def choose_shuffled_order(self): # Note: Images and labels must be added in shuffled order.\n",
    "        order = np.arange(self.expected_images)\n",
    "        np.random.RandomState(123).shuffle(order)\n",
    "        return order\n",
    "\n",
    "    def add_image(self, img):\n",
    "        if self.print_progress and self.cur_images % self.progress_interval == 0:\n",
    "            print('%d / %d\\r' % (self.cur_images, self.expected_images), end='', flush=True)\n",
    "        if self.shape is None:\n",
    "            self.shape = img.shape\n",
    "            self.resolution_log2 = int(np.log2(self.shape[1]))\n",
    "            assert self.shape[0] in [1, 3]\n",
    "            assert self.shape[1] == self.shape[2]\n",
    "            assert self.shape[1] == 2**self.resolution_log2\n",
    "            tfr_opt = tf.io.TFRecordOptions(compression_type = \"\")\n",
    "            for lod in range(self.resolution_log2 - 1):\n",
    "                tfr_file = self.tfr_prefix + '-r%02d.tfrecords' % (self.resolution_log2 - lod)\n",
    "                self.tfr_writers.append(tf.io.TFRecordWriter(tfr_file, tfr_opt))\n",
    "        assert img.shape == self.shape\n",
    "        for lod, tfr_writer in enumerate(self.tfr_writers):\n",
    "            if lod:\n",
    "                img = img.astype(np.float32)\n",
    "                img = (img[:, 0::2, 0::2] + img[:, 0::2, 1::2] + img[:, 1::2, 0::2] + img[:, 1::2, 1::2]) * 0.25\n",
    "            quant = np.rint(img).clip(0, 255).astype(np.uint8)\n",
    "            ex = tf.train.Example(features=tf.train.Features(feature={\n",
    "                'shape': tf.train.Feature(int64_list=tf.train.Int64List(value=quant.shape)),\n",
    "                'data': tf.train.Feature(bytes_list=tf.train.BytesList(value=[quant.tobytes()]))}))\n",
    "            tfr_writer.write(ex.SerializeToString())\n",
    "        self.cur_images += 1\n",
    "\n",
    "    def add_labels(self, labels):\n",
    "        if self.print_progress:\n",
    "            print('%-40s\\r' % 'Saving labels...', end='', flush=True)\n",
    "        assert labels.shape[0] == self.cur_images\n",
    "        with open(self.tfr_prefix + '-rxx.labels', 'wb') as f:\n",
    "            np.save(f, labels.astype(np.float32))\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98f31169-70ab-4d13-a3f2-afcbec13ac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_from_images(tfrecord_dir, image_dir, shuffle):\n",
    "    print('Loading images from \"%s\"' % image_dir)\n",
    "    image_filenames = sorted(glob.glob(os.path.join(image_dir, '*')))\n",
    "    if len(image_filenames) == 0:\n",
    "        error('No input images found')\n",
    "    \n",
    "    size = 256\n",
    "    dim = (size, size)\n",
    "\n",
    "    img = np.asarray(PIL.Image.open(image_filenames[0]))\n",
    "    resolution = img.shape[0]\n",
    "    channels = img.shape[2] if img.ndim == 3 else 1\n",
    "    if img.shape[1] != resolution:\n",
    "        error('Input images must have the same width and height')\n",
    "    if resolution != 2 ** int(np.floor(np.log2(resolution))):\n",
    "        error('Input image resolution must be a power-of-two')\n",
    "    if channels not in [1, 3]:\n",
    "        error('Input images must be stored as RGB or grayscale')\n",
    "\n",
    "    with TFRecordExporter(tfrecord_dir, len(image_filenames)) as tfr:\n",
    "        order = tfr.choose_shuffled_order() if shuffle else np.arange(len(image_filenames))\n",
    "        for idx in range(order.size):\n",
    "            img = np.asarray(PIL.Image.open(image_filenames[order[idx]]))\n",
    "            img = cv2.resize(img, dim, interpolation=cv2.INTER_AREA)\n",
    "            \n",
    "            if img.ndim == 3:\n",
    "                img = img[:, :, -1]\n",
    "                \n",
    "            if channels == 1:\n",
    "                img = img[np.newaxis, :, :] # HW => CHW\n",
    "            else:\n",
    "                img = img.transpose([2, 0, 1]) # HWC => CHW\n",
    "            tfr.add_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e8370ce-c833-450a-b376-5d75a357a350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images from \"/gpfs_projects/seif.younis/NIH Chest X-rays/images_001/images\"\n",
      "Creating dataset \"/gpfs_projects/seif.younis/NIH Chest X-rays/tfrecords/\"\n",
      "Added 4999 images.                      \n"
     ]
    }
   ],
   "source": [
    "create_from_images('/gpfs_projects/seif.younis/NIH Chest X-rays/tfrecords/', '/gpfs_projects/seif.younis/NIH Chest X-rays/images_001/images', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "194d46b2-a258-4a7e-92a3-a36d80321ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------\n",
    "# Dataset class that loads data from tfrecords files.\n",
    "\n",
    "class TFRecordDataset:\n",
    "    def __init__(self,\n",
    "        tfrecord_dir,               # Directory containing a collection of tfrecords files.\n",
    "        resolution      = None,     # Dataset resolution, None = autodetect.\n",
    "        label_file      = None,     # Relative path of the labels file, None = autodetect.\n",
    "        max_label_size  = 0,        # 0 = no labels, 'full' = full labels, <int> = N first label components.\n",
    "        max_images      = None,     # Maximum number of images to use, None = use all images.\n",
    "        repeat          = True,     # Repeat dataset indefinitely?\n",
    "        shuffle_mb      = 4096,     # Shuffle data within specified window (megabytes), 0 = disable shuffling.\n",
    "        prefetch_mb     = 2048,     # Amount of data to prefetch (megabytes), 0 = disable prefetching.\n",
    "        buffer_mb       = 256,      # Read buffer size (megabytes).\n",
    "        num_threads     = 2):       # Number of concurrent threads.\n",
    "\n",
    "        self.tfrecord_dir       = tfrecord_dir\n",
    "        self.resolution         = None\n",
    "        self.resolution_log2    = None\n",
    "        self.shape              = []        # [channels, height, width]\n",
    "        self.dtype              = 'uint8'\n",
    "        self.dynamic_range      = [0, 255]\n",
    "        self.label_file         = label_file\n",
    "        self.label_size         = None      # components\n",
    "        self.label_dtype        = None\n",
    "        self._np_labels         = None\n",
    "        self._tf_minibatch_in   = None\n",
    "        self._tf_labels_var     = None\n",
    "        self._tf_labels_dataset = None\n",
    "        self._tf_datasets       = dict()\n",
    "        self._tf_iterator       = None\n",
    "        self._tf_init_ops       = dict()\n",
    "        self._tf_minibatch_np   = None\n",
    "        self._cur_minibatch     = -1\n",
    "        self._cur_lod           = -1\n",
    "\n",
    "        # List tfrecords files and inspect their shapes.\n",
    "        assert os.path.isdir(self.tfrecord_dir)\n",
    "        tfr_files = sorted(glob.glob(os.path.join(self.tfrecord_dir, '*.tfrecords')))\n",
    "        assert len(tfr_files) >= 1\n",
    "        tfr_shapes = []\n",
    "        for tfr_file in tfr_files:\n",
    "            data=tf.data.TFRecordDataset(tfr_file) \n",
    "            # tfr_opt = tf.io.TFRecordOptions(compression_type='')\n",
    "            # for record in tf.io.tf_record_iterator(tfr_file, tfr_opt):\n",
    "            for record in data:\n",
    "                tfr_shapes.append(self.parse_tfrecord_np(record).shape)\n",
    "                break\n",
    "\n",
    "        # Autodetect label filename.\n",
    "        if self.label_file is None:\n",
    "            guess = sorted(glob.glob(os.path.join(self.tfrecord_dir, '*.labels')))\n",
    "            if len(guess):\n",
    "                self.label_file = guess[0]\n",
    "        elif not os.path.isfile(self.label_file):\n",
    "            guess = os.path.join(self.tfrecord_dir, self.label_file)\n",
    "            if os.path.isfile(guess):\n",
    "                self.label_file = guess\n",
    "\n",
    "        # Determine shape and resolution.\n",
    "        max_shape = max(tfr_shapes, key=np.prod)\n",
    "        self.resolution = resolution if resolution is not None else max_shape[1]\n",
    "        self.resolution_log2 = int(np.log2(self.resolution))\n",
    "        self.shape = [max_shape[0], self.resolution, self.resolution]\n",
    "        tfr_lods = [self.resolution_log2 - int(np.log2(shape[1])) for shape in tfr_shapes]\n",
    "        assert all(shape[0] == max_shape[0] for shape in tfr_shapes)\n",
    "        assert all(shape[1] == shape[2] for shape in tfr_shapes)\n",
    "        assert all(shape[1] == self.resolution // (2**lod) for shape, lod in zip(tfr_shapes, tfr_lods))\n",
    "        assert all(lod in tfr_lods for lod in range(self.resolution_log2 - 1))\n",
    "\n",
    "        # Load labels.\n",
    "        assert max_label_size == 'full' or max_label_size >= 0\n",
    "        self._np_labels = np.zeros([1<<30, 0], dtype=np.float32)\n",
    "        if self.label_file is not None and max_label_size != 0:\n",
    "            self._np_labels = np.load(self.label_file)\n",
    "            assert self._np_labels.ndim == 2\n",
    "        if max_label_size != 'full' and self._np_labels.shape[1] > max_label_size:\n",
    "            self._np_labels = self._np_labels[:, :max_label_size]\n",
    "        if max_images is not None and self._np_labels.shape[0] > max_images:\n",
    "            self._np_labels = self._np_labels[:max_images]\n",
    "        self.label_size = self._np_labels.shape[1]\n",
    "        self.label_dtype = self._np_labels.dtype.name\n",
    "\n",
    "        # Build TF expressions.\n",
    "        with tf.name_scope('Dataset'), tf.device('/cpu:0'):\n",
    "            # self._tf_minibatch_in = tf.placeholder(tf.int64, name='minibatch_in', shape=[])\n",
    "            # self._tf_labels_var = create_var_with_large_initial_value(self._np_labels, name='labels_var')\n",
    "            # self._tf_labels_dataset = tf.data.Dataset.from_tensor_slices(self._tf_labels_var)\n",
    "            for tfr_file, tfr_shape, tfr_lod in zip(tfr_files, tfr_shapes, tfr_lods):\n",
    "                if tfr_lod < 0:\n",
    "                    continue\n",
    "                dset = tf.data.TFRecordDataset(tfr_file, compression_type='', buffer_size=buffer_mb<<20)\n",
    "                if max_images is not None:\n",
    "                    dset = dset.take(max_images)\n",
    "                dset = dset.map(self.parse_tfrecord_tf, num_parallel_calls=num_threads)\n",
    "                # dset = tf.data.Dataset.zip((dset, self._tf_labels_dataset))\n",
    "                bytes_per_item = np.prod(tfr_shape) * np.dtype(self.dtype).itemsize\n",
    "                if shuffle_mb > 0:\n",
    "                    dset = dset.shuffle(((shuffle_mb << 20) - 1) // bytes_per_item + 1)\n",
    "                if repeat:\n",
    "                    dset = dset.repeat()\n",
    "                if prefetch_mb > 0:\n",
    "                    dset = dset.prefetch(((prefetch_mb << 20) - 1) // bytes_per_item + 1)\n",
    "                # dset = dset.batch(self._tf_minibatch_in)\n",
    "                self._tf_datasets[tfr_lod] = dset\n",
    "            print(type(self._tf_datasets[0].output_types))\n",
    "            print(type(self._tf_datasets[0].output_shapes))\n",
    "            self._tf_iterator = tf.data.Iterator.from_structure(self._tf_datasets[0].output_types, self._tf_datasets[0].output_shapes)\n",
    "            self._tf_init_ops = {lod: self._tf_iterator.make_initializer(dset) for lod, dset in self._tf_datasets.items()}\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    # Use the given minibatch size and level-of-detail for the data returned by get_minibatch_tf().\n",
    "    def configure(self, minibatch_size, lod=0):\n",
    "        lod = int(np.floor(lod))\n",
    "        assert minibatch_size >= 1 and lod in self._tf_datasets\n",
    "        if self._cur_minibatch != minibatch_size or self._cur_lod != lod:\n",
    "            self._tf_init_ops[lod].run({self._tf_minibatch_in: minibatch_size})\n",
    "            self._cur_minibatch = minibatch_size\n",
    "            self._cur_lod = lod\n",
    "\n",
    "    # Get next minibatch as TensorFlow expressions.\n",
    "    def get_minibatch_tf(self): # => images, labels\n",
    "        return self._tf_iterator.get_next()\n",
    "\n",
    "    # Get next minibatch as NumPy arrays.\n",
    "    def get_minibatch_np(self, minibatch_size, lod=0): # => images, labels\n",
    "        self.configure(minibatch_size, lod)\n",
    "        with tf.name_scope('Dataset'):\n",
    "            if self._tf_minibatch_np is None:\n",
    "                self._tf_minibatch_np = self.get_minibatch_tf()\n",
    "            return tflib.run(self._tf_minibatch_np)\n",
    "\n",
    "    # Get random labels as TensorFlow expression.\n",
    "    def get_random_labels_tf(self, minibatch_size): # => labels\n",
    "        with tf.name_scope('Dataset'):\n",
    "            if self.label_size > 0:\n",
    "                with tf.device('/cpu:0'):\n",
    "                    return tf.gather(self._tf_labels_var, tf.random_uniform([minibatch_size], 0, self._np_labels.shape[0], dtype=tf.int32))\n",
    "            return tf.zeros([minibatch_size, 0], self.label_dtype)\n",
    "\n",
    "    # Get random labels as NumPy array.\n",
    "    def get_random_labels_np(self, minibatch_size): # => labels\n",
    "        if self.label_size > 0:\n",
    "            return self._np_labels[np.random.randint(self._np_labels.shape[0], size=[minibatch_size])]\n",
    "        return np.zeros([minibatch_size, 0], self.label_dtype)\n",
    "\n",
    "    # Parse individual image from a tfrecords file into TensorFlow expression.\n",
    "    @staticmethod\n",
    "    def parse_tfrecord_tf(record):\n",
    "        features = tf.io.parse_single_example(record, features={\n",
    "            'shape': tf.io.FixedLenFeature([3], tf.int64),\n",
    "            'data': tf.io.FixedLenFeature([], tf.string)})\n",
    "        data = tf.io.decode_raw(features['data'], tf.uint8)\n",
    "        return tf.reshape(data, features['shape'])\n",
    "\n",
    "    # Parse individual image from a tfrecords file into NumPy array.\n",
    "    @staticmethod\n",
    "    def parse_tfrecord_np(record):\n",
    "        ex = tf.train.Example()\n",
    "        ex.ParseFromString(record.numpy())\n",
    "        shape = ex.features.feature['shape'].int64_list.value # pylint: disable=no-member\n",
    "        data = ex.features.feature['data'].bytes_list.value[0] # pylint: disable=no-member\n",
    "        return np.frombuffer(data, np.uint8).reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0160e5bd-bd24-4075-9cc6-4c28d538e7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(tfrecord_dir):\n",
    "    print('Loading dataset \"%s\"' % tfrecord_dir)\n",
    "    # tflib.init_tf({'gpu_options.allow_growth': True})\n",
    "    dset = TFRecordDataset(tfrecord_dir, max_label_size=0, repeat=False, shuffle_mb=0)\n",
    "    # tflib.init_uninitialized_vars()\n",
    "    import cv2  # pip install opencv-python\n",
    "\n",
    "    idx = 0\n",
    "    while True:\n",
    "        try:\n",
    "            images, labels = dset.get_minibatch_np(1)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "        if idx == 0:\n",
    "            print('Displaying images')\n",
    "            cv2.namedWindow('dataset_tool')\n",
    "            print('Press SPACE or ENTER to advance, ESC to exit')\n",
    "        print('\\nidx = %-8d\\nlabel = %s' % (idx, labels[0].tolist()))\n",
    "        cv2.imshow('dataset_tool', images[0].transpose(1, 2, 0)[:, :, ::-1]) # CHW => HWC, RGB => BGR\n",
    "        idx += 1\n",
    "        if cv2.waitKey() == 27:\n",
    "            break\n",
    "    print('\\nDisplayed %d images.' % idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92d500d1-bffa-4730-b94e-ef0fa2be8ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset \"/gpfs_projects/seif.younis/NIH Chest X-rays/tfrecords/\"\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.ParallelMapDataset'>\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.ParallelMapDataset'>\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.ParallelMapDataset'>\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.ParallelMapDataset'>\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.ParallelMapDataset'>\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.ParallelMapDataset'>\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.ParallelMapDataset'>\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.ParallelMapDataset'>\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.ParallelMapDataset'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'IteratorBase' has no attribute 'from_structure'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/gpfs_projects/seif.younis/NIH Chest X-rays/tfrecords/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mdisplay\u001b[0;34m(tfrecord_dir)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading dataset \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m tfrecord_dir)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# tflib.init_tf({'gpu_options.allow_growth': True})\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m dset \u001b[38;5;241m=\u001b[39m \u001b[43mTFRecordDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtfrecord_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_label_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle_mb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# tflib.init_uninitialized_vars()\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m  \u001b[38;5;66;03m# pip install opencv-python\u001b[39;00m\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mTFRecordDataset.__init__\u001b[0;34m(self, tfrecord_dir, resolution, label_file, max_label_size, max_images, repeat, shuffle_mb, prefetch_mb, buffer_mb, num_threads)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# dset = dset.batch(self._tf_minibatch_in)\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tf_datasets[tfr_lod] \u001b[38;5;241m=\u001b[39m dset\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tf_iterator \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIterator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_structure\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tf_datasets[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39moutput_types, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tf_datasets[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39moutput_shapes)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tf_init_ops \u001b[38;5;241m=\u001b[39m {lod: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tf_iterator\u001b[38;5;241m.\u001b[39mmake_initializer(dset) \u001b[38;5;28;01mfor\u001b[39;00m lod, dset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tf_datasets\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'IteratorBase' has no attribute 'from_structure'"
     ]
    }
   ],
   "source": [
    "display('/gpfs_projects/seif.younis/NIH Chest X-rays/tfrecords/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906696e4-aebf-4182-bc14-4e7b47b90d18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
